<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Please delete this script if you use this HTML. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-7580334-1');
  </script>
  <meta name="viewport" content="width=500; text/html; charset=utf-8" http-equiv="Content-Type">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <title>Junke Wang @ FDU</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Junke Wang 「王君可」 </name>
              </p>

              <p> I'm a final year Ph.D. student at Fudan University, supervised by <a
                  href="http://zxwu.azurewebsites.net" target="_blank">Prof. Zuxuan Wu</a> and <a
                  href="https://fvl.fudan.edu.cn" target="_blank">Prof. Yu-Gang Jiang</a>. My research interest lies in computer vision and deep learning, with the emphasis on <strong>multimodal general intelligence</strong>. </br></br>
              
              I developed Omni-series models, including <a href="https://arxiv.org/abs/2406.09399">OmniTokenizer</a> (one codebook for image-video joint tokenization), <a href="https://arxiv.org/abs/2403.17935">OmniVid</a> (a generative framework for general video understanding), <a href="https://arxiv.org/abs/2303.12079">OmniTracker</a> (a unified tracking model), and <a href="https://arxiv.org/abs/2209.07526">OmniVL</a> (an image-video-language foundation model). </br></br>
              
              I am a devoted believer of llya: predicting next token well enough can lead to intelligence.
            </p>

            <p>
              Email: wangjk21[at]m.fudan.edu.cn
            </p>

              <p align=left>
                <a href="https://scholar.google.com/citations?user=qQuxuo0AAAAJ&hl=en" target="_blank">
                  Google Scholar</a> &nbsp&nbsp/&nbsp&nbsp

                <a href="https://github.com/wdrink" target="_blank">
                  Github</a>

              </p>
            </td>
            <td style="padding:2.5%;width:100%;max-width:100%">
              <a href="images/wangjunke.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/wangjunke.jpg"
                  class="hoverZoomLink"></a>
            </td>
          </tr>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publication</heading>
            </td>

            <br>
            (* denotes equal contribution)

          </tr>
      
          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="">
                <papertitle>OmniGen-AR: AutoRegressive Any-to-Image Generation. </papertitle> 
              </a>
              <br>
              <strong>Junke Wang</strong>, Xun Wang, Qiushan Guo, Peize Sun, Weilin Huang, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>NeurIPS</em>, 2025.
              <br>
            </td>
          </tr>

           <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2504.13181">
                <papertitle>Perception Encoder: The best visual embeddings are not at the output of the network. <a href="https://github.com/facebookresearch/perception_models">
                  [Code]
                </a>
                </papertitle>
              </a>
              <br>
              PE Team from FAIR, Meta.
              <br>
              <em>NeurIPS</em>, 2025 (<strong><em>Oral</em></strong>).
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2406.09399">
                <papertitle>OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation. </papertitle> <a href="https://github.com/FoundationVision/OmniTokenizer">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang</strong>, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>NeurIPS</em>, 2024.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2507.01756">
                <papertitle>Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis. </papertitle>
              </a>
              <br>
              Peng Zheng, <strong>Junke Wang</strong>, Yi Chang, Yizhou Yu, Rui Ma, Zuxuan Wu.
              <br>
              <em>ICCV</em>, 2025.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2212.05667">
                <papertitle>Fighting Malicious Media Data: A Survey on Tampering Detection and Deepfake Detection.
                </papertitle>
              </a>
              <br>
              <strong>Junke Wang</strong>, Zhenxin Li, Chao Zhang, Jingjing Chen, Zuxuan Wu, Larry S. Davis, Yu-Gang
              Jiang.
              <br>
              <em>Proceedings of IEEE</em>, 2025.
              <br>
            </td>
          </tr>
          
          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2303.12079">
                <papertitle>OmniTracker: Unifying Object Tracking by Tracking-with-Detection.
                </papertitle>
              </a>
              <br>
              <strong>Junke Wang</strong>, Dongdong Chen, Zuxuan Wu, Chong Luo, Xiyang Dai, Lu Yuan, Yu-Gang Jiang.
              <br>
              <em>TPAMI</em>, 2025.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2403.17935">
                <papertitle>OmniVid: A Generative Framework for Universal Video Understanding.
                </papertitle> <a href="https://github.com/wdrink/OmniVid">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang</strong>, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>CVPR</em>, 2024.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
              <td valign="middle" width="100%">
                <a href="https://arxiv.org/abs/2212.06826">
                  <papertitle>Look Before You Match: Instance Understanding Matters in Video Object Segmentation.
                  </papertitle>
                </a>
                <br>
                <strong>Junke Wang</strong>, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao, <br> Yujia Xie, Lu Yuan, Yu-Gang Jiang.
                <br>
                <em>CVPR</em>, 2023.
                <br>
              </td>
            </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2209.07526">
                <papertitle>OmniVL: One Foundation Model for Image-Language and Video-Language Tasks.</papertitle>
              </a>
              <br>
              <strong>Junke Wang</strong>, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, <br> Yujia Xie, Ce
              Liu, Yu-Gang Jiang, Lu Yuan.
              <br>
              <em>NeurIPS</em>, 2022.
              <br>
            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2111.11591">
                <papertitle>Efficient Video Transformers with Spatial-Temporal Token Selection.</papertitle> 
                <a href="https://github.com/wdrink/STTS">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang*</strong>, Xitong Yang*, Hengduo Li, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>ECCV</em>, 2022.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2104.09770">
                <papertitle>M2TR: Multi-modal Multi-scale Transformer for Deepfake Detection.</papertitle>
                <a href="https://github.com/wdrink/M2TR-Multi-modal-Multi-scale-Transformers-for-Deepfake-Detection">
                  [Code]
                </a>

              </a>
              <br>
              <strong>Junke Wang</strong>, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Ser-Nam Lim, Yu-Gang
              Jiang
              <br>
              <em>ICMR</em>, 2022.
              <br>
            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2203.14681">
                <papertitle>ObjectFormer for Image Manipulation Detection and Localization. </papertitle>
                <a href="https://github.com/wdrink/Objectformer">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang</strong>, Zuxuan Wu, Jingjing Chen, Xintong Han, Abhinav Shrivastava, Yu-Gang Jiang,
              Ser-Nam Li.
              <br>
              <em>CVPR</em>, 2022.
              <br>

            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2108.04424">
                <papertitle>FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face
                  Inpainting.</papertitle>
              </a>
              <br>
              <strong>Junke Wang</strong>, Shaoxiang Chen, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>TMM</em>, 2022.
              <br>
            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2010.09982">
                <papertitle>Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition. </papertitle>
              </a>
              <br>
              Yuqian Fu, Li Zhang, <strong>Junke Wang</strong>, Yanwei Fu, Yu-Gang Jiang.
              <br>
              <em>ACM MM</em>, 2020.
              <br>
            </td>
          </tr>
        </table>
        <br>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="middle">
              <heading>Preprints</heading>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2501.13893">
                <papertitle>Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning.
                </papertitle>
              </a>
              <br>
              Zuyao You*, <strong>Junke Wang*</strong>, Lingyu Kong, Bo He, Zuxuan Wu
              <br>
              <em>Arxiv</em>, 2025.
              <br>
            </td>
          </tr>

        </table>

          
          <br>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Projects</heading>
                </td>
              </tr>

              <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2504.11455">
                <papertitle>SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL.
                </papertitle> <a href="https://github.com/wdrink/SimpleAR">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang</strong>, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, Yu-Gang Jiang
              <p style="padding-left: 1em;"> <li> We train a state-of-the-art autoregressive image generation model, outperforming diffusion models with only 0.5B/1.5B parameters. For the first time, we verify the effectiveness of GRPO in visual generation.</li> </p>
              
            </td>
          </tr>

                <td valign="middle" width="100%">
                  <a href="https://arxiv.org/abs/2311.07574">
                    <papertitle>To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning.</papertitle> <a href="https://huggingface.co/datasets/X2FD/LVIS-Instruct4V">
                      [Dataset]
                    </a>
                    <a href="https://github.com/X2FD/LVIS-INSTRUCT4V">
                      [Project page]
                    </a>
                  </a>
                  <br>
                  <strong>Junke Wang*</strong>, Lingchen Meng*, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang.
                  <p style="padding-left: 1em;"> <li> We introduce a fine-grained visual instruction dataset, LVIS-INSTRUCT4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS.</li> </p>

                </td>
              </tr>

              <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
                <td valign="middle" width="100%">
                  <a href="https://arxiv.org/abs/2304.14407">
                    <papertitle>ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System.</papertitle> <a href="https://github.com/yiwengxie/Chat-Video">
                      [Code]
                    </a>
                  </a>
                  <br>
                  <strong>Junke Wang</strong>, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang.
                  <br>
                    <p style="padding-left: 1em;"> <li> We present our vision for multimodal and versatile video understanding and propose a prototype system, ChatVideo. </li> </p>
                  <br>
                </td>
              </tr>
            </table>

              
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                <tr>
                  <td width="100%" valign="middle">
                    <heading>Academic Services</heading>

                    <p>
                      Conference Reviewer for CVPR, ICCV, ICML, NeurIPS, ICLR, ECCV, etal.
                    </p>
		                <p>
                      Journal Reviewer for TPAMI, TIP, IJCV, etal.
                    </p>



                  </td>
                </tr>
              </table>


              <br>
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                <tr>
                  <td width="100%" valign="middle">
                    <heading>Selected Awards</heading>
                    <p>
                      Academic Star in Fudan University (10 PhD students). 2025.
                    </p>
                    <p>
                      Fundamental Research Program for PhD students, sponsored by NSFC. 2024.
                    </p>
                    <p>
                      Young Elite Scientists Sponsorship Program for PhD students, sponsored by CAAI. 2024.
                    </p>
                    <p>
                      Intel Fellowship. 2023.
                    </p>
                    <p>
                      National Scholarship (Top 1%). 2022.
                    </p>
                    <p>
                      Outstanding graduates in Shanghai (undergrads). 2021.
                    </p>
                  </td>
                </tr>
              </table>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <p font-size:small;>
                        <br>
                        <br>
                      <div style="float:left;">
                        Updated at Sep 2025.
                      </div>
                      <div style="float:right;">
                        <a href="https://jonbarron.info">Template</a>
                      </div>
                      <br>
                      <br>
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
      </td>
    </tr>
  </table>
</body>

</html>
