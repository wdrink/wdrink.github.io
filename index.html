<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Please delete this script if you use this HTML. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-7580334-1');
  </script>
  <meta name="viewport" content="width=500; text/html; charset=utf-8" http-equiv="Content-Type">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <title>Junke Wang @ FDU</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Junke Wang 「王君可」 </name>
              </p>

              <p> I'm a 4th year Ph.D. student in school of computer science at Fudan University, supervised by <a
                  href="http://zxwu.azurewebsites.net" target="_blank">Prof. Zuxuan Wu</a> and <a
                  href="http://www.yugangjiang.info" target="_blank">Prof. Yu-Gang Jiang</a>. Before that, I received my
                Bachelor's degree of Computer Science at Fudan University in 2021. During my Ph.D. study, I was very fortunate to be mentored by <a
                href="http://www.dongdongchen.bid" target="_blank">Dongdong Chen</a> and <a
                href="https://enjoyyi.github.io" target="_blank">Yi Jiang</a>.</br></br>

                My research interest lies in computer vision and deep learning, with the emphasis on multimodal understanding and generation. </br></br> 
                
                My long-term goal is to train <strong>fundamental, universal, and scalable</strong> vision (+language) models, the early attempts are Omni-series, including <a href="https://arxiv.org/abs/2406.09399">OmniTokenizer</a> (one codebook for image-video joint tokenization), <a href="https://arxiv.org/abs/2403.17935">OmniVid</a> (a generative framework that supports various video tasks), <a href="https://arxiv.org/abs/2209.07526">OmniVL</a> (an image-video-language foundation model), and <a href="https://arxiv.org/abs/2303.12079">OmniTracker</a> (a unified tracking model). Based on the above work, I developed the first interactive multimodal video understanding system, <a href="https://arxiv.org/abs/2304.14407">ChatVideo</a>.

              </p>
              <p> Email: wangjk21 [at] m.fudan.edu.cn
              </p>
              <p align=left>
                <a href="https://scholar.google.com/citations?user=qQuxuo0AAAAJ&hl=en" target="_blank">
                  Google Scholar</a> &nbsp&nbsp/&nbsp&nbsp

                <a href="files/Resume.pdf" target="_blank">
                  CV</a> &nbsp&nbsp/&nbsp&nbsp


                <a href="https://github.com/wdrink" target="_blank">
                  Github</a> &nbsp&nbsp&nbsp&nbsp



              </p>
            </td>
            <td style="padding:2.5%;width:100%;max-width:100%">
              <a href="images/wjk.png"><img style="width:70%;max-width:100%" alt="profile photo" src="images/wjk.png"
                  class="hoverZoomLink"></a>
            </td>
          </tr>
        </table>





        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publication</heading>
            </td>
          </tr>


          <br>
          (* denotes equal contribution)

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2406.09399">
                <papertitle>OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation. </papertitle> <a href="https://github.com/FoundationVision/OmniTokenizer">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang</strong>, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>NeurIPS</em>, 2024.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2403.17935">
                <papertitle>OmniVid: A Generative Framework for Universal Video Understanding.
                </papertitle> <a href="https://github.com/wdrink/OmniVid">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang</strong>, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>CVPR</em>, 2024.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2401.17221">
                <papertitle>MouSi: Poly-Visual-Expert Vision-Language Models.
                </papertitle>
              </a>
              <br>
              Xiaoran Fan*, Tao Ji*, Changhao Jiang*, Shuo Li*, Senjie Jin*, Sirui Song, <strong>Junke Wang</strong>, etc.
              <br>
              <em>COLM</em>, 2024.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
              <td valign="middle" width="100%">
                <a href="https://arxiv.org/abs/2212.06826">
                  <papertitle>Look Before You Match: Instance Understanding Matters in Video Object Segmentation.
                  </papertitle>
                </a>
                <br>
                <strong>Junke Wang</strong>, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao, <br> Yujia Xie, Lu Yuan, Yu-Gang Jiang.
                <br>
                <em>CVPR</em>, 2023.
                <br>
              </td>
            </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2209.07526">
                <papertitle>OmniVL: One Foundation Model for Image-Language and Video-Language Tasks.</papertitle>
              </a>
              <br>
              <strong>Junke Wang</strong>, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, <br> Yujia Xie, Ce
              Liu, Yu-Gang Jiang, Lu Yuan.
              <br>
              <em>NeurIPS</em>, 2022.
              <br>
            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2111.11591">
                <papertitle>Efficient Video Transformers with Spatial-Temporal Token Selection.</papertitle> 
                <a href="https://github.com/wdrink/STTS">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang*</strong>, Xitong Yang*, Hengduo Li, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>ECCV</em>, 2022.
              <br>
            </td>
          </tr>

          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2104.09770">
                <papertitle>M2TR: Multi-modal Multi-scale Transformer for Deepfake Detection.</papertitle>
                <a href="https://github.com/wdrink/M2TR-Multi-modal-Multi-scale-Transformers-for-Deepfake-Detection">
                  [Code]
                </a>

              </a>
              <br>
              <strong>Junke Wang</strong>, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Ser-Nam Lim, Yu-Gang
              Jiang
              <br>
              <em>ICMR</em>, 2022.
              <br>
            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2203.14681">
                <papertitle>ObjectFormer for Image Manipulation Detection and Localization. </papertitle>
                <a href="https://github.com/wdrink/Objectformer">
                  [Code]
                </a>
              </a>
              <br>
              <strong>Junke Wang</strong>, Zuxuan Wu, Jingjing Chen, Xintong Han, Abhinav Shrivastava, Yu-Gang Jiang,
              Ser-Nam Li.
              <br>
              <em>CVPR</em>, 2022.
              <br>

            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2108.04424">
                <papertitle>FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face
                  Inpainting.</papertitle>
              </a>
              <br>
              <strong>Junke Wang</strong>, Shaoxiang Chen, Zuxuan Wu, Yu-Gang Jiang.
              <br>
              <em>TMM</em>, 2022.
              <br>
            </td>
          </tr>


          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td valign="middle" width="100%">
              <a href="https://arxiv.org/abs/2010.09982">
                <papertitle>Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition. </papertitle>
              </a>
              <br>
              Yuqian Fu, Li Zhang, <strong>Junke Wang</strong>, Yanwei Fu, Yu-Gang Jiang.
              <br>
              <em>ACM MM</em>, 2020.
              <br>
            </td>
          </tr>


          <br>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
            <tr>
              <td width="100%" valign="middle">
                <heading>Preprints</heading>
              </td>
            </tr>

	          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
              <td valign="middle" width="100%">
                <a href="https://arxiv.org/abs/2303.12079">
                  <papertitle>OmniTracker: Unifying Object Tracking by Tracking-with-Detection.
                  </papertitle>
                </a>
                <br>
                <strong>Junke Wang</strong>, Dongdong Chen, Zuxuan Wu, Chong Luo, Xiyang Dai, Lu Yuan, Yu-Gang Jiang.
                <br>
                <em>Arxiv</em>, 2023.
                <br>
              </td>
            </tr>

            <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
              <td valign="middle" width="100%">
                <a href="https://arxiv.org/abs/2212.05667">
                  <papertitle>Fighting Malicious Media Data: A Survey on Tampering Detection and Deepfake Detection.
                  </papertitle>
                </a>
                <br>
                <strong>Junke Wang</strong>, Zhenxin Li, Chao Zhang, Jingjing Chen, Zuxuan Wu, Larry S. Davis, Yu-Gang
                Jiang.
                <br>
                <em>Arxiv</em>, 2022.
                <br>
              </td>
            </tr>



            <br>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Projects</heading>
                </td>
              </tr>

	       <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
                <td valign="middle" width="100%">
                  <a href="https://arxiv.org/abs/2311.07574">
                    <papertitle>To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning.</papertitle> <a href="https://huggingface.co/datasets/X2FD/LVIS-Instruct4V">
                      [Dataset]
                    </a>
                    <a href="https://github.com/X2FD/LVIS-INSTRUCT4V">
                      [Project page]
                    </a>
                  </a>
                  <br>
                  <strong>Junke Wang*</strong>, Lingchen Meng*, Zejia Weng, Bo He, Zuxuan Wu, Yu-Gang Jiang.
                  <p style="padding-left: 1em;"> <li> We introduce a fine-grained visual instruction dataset, LVIS-INSTRUCT4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS.</li> </p>

                </td>
              </tr>

              <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
                <td valign="middle" width="100%">
                  <a href="https://arxiv.org/abs/2304.14407">
                    <papertitle>ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System.</papertitle> <a href="https://github.com/yiwengxie/Chat-Video">
                      [Code]
                    </a>
                  </a>
                  <br>
                  <strong>Junke Wang</strong>, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang.
                  <br>
                    <p style="padding-left: 1em;"> <li> We present our vision for multimodal and versatile video understanding and propose a prototype system, ChatVideo. </li> </p>
                  <br>
                </td>
              </tr>


              <br>
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                <tr>
                  <td width="100%" valign="middle">
                    <heading>Academic Services</heading>

                    <p>
                      Conference Reviewer for </strong><strong>CVPR</strong>, <strong>ICCV</strong>, <strong>NeurIPS</strong>, <strong>ICLR</strong>, <strong>ECCV</strong>, etal.
                    </p>
		                <p>
                      Journal Reviewer for <strong>TPAMI</strong>,  <strong>TIP</strong>, <strong>IJCV</strong>, <strong>TMM</strong>, etal.
                    </p>



                  </td>
                </tr>
              </table>


              <br>
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                <tr>
                  <td width="100%" valign="middle">
                    <heading>Awards</heading>
                    <p>
                      Intel Scholarship (5 graudates in Fudan University). 2023.
                    </p>
                    <p>
                      National Scholarship (Top 1%). 2022.
                    </p>
                    <p>
                      Outstanding graduates in Shanghai (undergrads). 2021.
                    </p>
                    <p>
                      First-class Scholarship (Top 5%). 2019, 2021.
                    </p>
                    <p>
                      Uniqlo Scholarship (33 undergrads from China). 2019.
                    </p>
                  </td>
                </tr>
              </table>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <p font-size:small;>
                        <br>
                        <br>
                      <div style="float:left;">
                        Updated at Jul. 2024.
                      </div>
                      <div style="float:right;">
                        <a href="https://jonbarron.info">Template</a>
                      </div>
                      <br>
                      <br>
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
      </td>
    </tr>
  </table>
</body>

</html>
